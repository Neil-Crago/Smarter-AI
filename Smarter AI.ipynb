{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b9956d4-dbc8-4387-b33f-4e0c27a4392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import tqdm as notebook_tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def text_to_granular_signal(text_response: str, window_size: int = 10, step_size: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts a text string into a 1D numerical signal using a\n",
    "    sliding window of words for higher granularity.\n",
    "    \"\"\"\n",
    "    words = text_response.split()\n",
    "    \n",
    "    if len(words) <= window_size:\n",
    "        # Text is too short to create a meaningful signal\n",
    "        return np.array([])\n",
    "\n",
    "    # Create chunks of text using the sliding window\n",
    "    chunks = []\n",
    "    for i in range(0, len(words) - window_size + 1, step_size):\n",
    "        chunk = \" \".join(words[i:i+window_size])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    if len(chunks) < 2:\n",
    "        return np.array([])\n",
    "        \n",
    "    # 1. Vectorize: Get the embedding for each chunk\n",
    "    embeddings = model.encode(chunks)\n",
    "    \n",
    "    # 2. Signalize: Calculate cosine similarity between adjacent chunk vectors\n",
    "    similarities = []\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        vec1 = embeddings[i]\n",
    "        vec2 = embeddings[i+1]\n",
    "        similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        similarities.append(similarity)\n",
    "        \n",
    "    return np.array(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16b661fe-f697-4414-9536-295426cc15fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence_score(signal: np.ndarray) -> float:\n",
    "    \"\"\"Applies FFT and calculates a coherence score.\"\"\"\n",
    "    if signal.size == 0:\n",
    "        return 0.0\n",
    "    fft_result = np.fft.fft(signal)\n",
    "    n = signal.size\n",
    "    power_spectrum = np.abs(fft_result[:n//2])\n",
    "    low_freq_cutoff = int(len(power_spectrum) * 0.2)\n",
    "    low_freq_power = np.sum(power_spectrum[:low_freq_cutoff])\n",
    "    high_freq_power = np.sum(power_spectrum[low_freq_cutoff:])\n",
    "    total_power = low_freq_power + high_freq_power\n",
    "    if total_power == 0:\n",
    "        return 0.0\n",
    "    return low_freq_power / total_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85423205-5756-4df2-ab8e-c45968980e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signal_from_chunks(chunks: list) -> np.ndarray:\n",
    "    \"\"\"Encodes a list of text chunks and calculates their similarity signal.\"\"\"\n",
    "    if len(chunks) < 2:\n",
    "        return np.array([])\n",
    "    \n",
    "    embeddings = model.encode(chunks)\n",
    "    \n",
    "    similarities = []\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        vec1 = embeddings[i]\n",
    "        vec2 = embeddings[i+1]\n",
    "        similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        similarities.append(similarity)\n",
    "        \n",
    "    return np.array(similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "640e69ac-ed22-4c0d-87f7-0c6fdb239dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_compound_score(text_response: str, weights: dict = None) -> list:\n",
    "    \"\"\"\n",
    "    Calculates a compound coherence score by analyzing the text at\n",
    "    multiple levels of granularity.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        # Default weights if none are provided\n",
    "        weights = {'sentence': 0.4, 'long': 0.3, 'medium': 0.2, 'low': 0.1}\n",
    "\n",
    "    words = text_response.split()\n",
    "    word_count = len(words)\n",
    "    \n",
    "    if word_count < 10: # Set a minimum length for meaningful analysis\n",
    "        return 0.0\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    # ---- 1. Sentence-level analysis ----\n",
    "    sentences = [s for s in text_response.split('.') if s.strip()]\n",
    "    sentence_signal = get_signal_from_chunks(sentences)\n",
    "    scores['sentence'] = calculate_coherence_score(sentence_signal)\n",
    "\n",
    "    # ---- 2. Phrase-level analysis ----\n",
    "    # Define granularities as percentages of the total word count\n",
    "    phrase_levels = {\n",
    "        'long': 0.20,\n",
    "        'medium': 0.10,\n",
    "        'low': 0.05\n",
    "    }\n",
    "\n",
    "    for level_name, percentage in phrase_levels.items():\n",
    "        window_size = int(word_count * percentage)\n",
    "        if window_size < 2: # Ensure window is not too small\n",
    "            scores[level_name] = 0.0\n",
    "            continue\n",
    "        \n",
    "        # Create chunks using a sliding window\n",
    "        chunks = []\n",
    "        for i in range(word_count - window_size + 1):\n",
    "            chunk = \" \".join(words[i:i+window_size])\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "        phrase_signal = get_signal_from_chunks(chunks)\n",
    "        scores[level_name] = calculate_coherence_score(phrase_signal)\n",
    "        \n",
    "    # ---- 3. Aggregate into a final compound score ----\n",
    "    final_score = [scores['sentence'],\n",
    "                   scores['long'],\n",
    "                   scores['medium'],\n",
    "                   scores['low']]\n",
    "    \n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09b91db9-edc9-40f4-acc0-93f7e9126f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pronouncing\n",
    "import re\n",
    "\n",
    "# Assume our robust syllable function is defined\n",
    "def get_syllable_count_robust(word: str) -> int:\n",
    "    \"\"\"\n",
    "    Calculates a reliable syllable count for a word by counting\n",
    "    the stress markers in its phonetic representation.\n",
    "    \"\"\"\n",
    "    # Get the list of possible pronunciations\n",
    "    phones_list = pronouncing.phones_for_word(word)\n",
    "    \n",
    "    # If the word is not in the dictionary, return 0\n",
    "    if not phones_list:\n",
    "        return 0\n",
    "        \n",
    "    # Use the first pronunciation in the list\n",
    "    first_pronunciation = phones_list[0]\n",
    "    \n",
    "    # Count the number of items that contain a digit (0, 1, or 2)\n",
    "    # These are the vowels, and their count is the syllable count.\n",
    "    syllable_count = len(re.findall(r'\\d', first_pronunciation))\n",
    "    \n",
    "    return syllable_count\n",
    "    \n",
    "def calculate_rhythm_score(text_response: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates a score based on the prevalence of the iambic (unstressed-stressed)\n",
    "    pattern in the text.\n",
    "    \"\"\"\n",
    "    words = text_response.lower().split()\n",
    "    \n",
    "    # 1. Get the full stress pattern for the entire response\n",
    "    full_stress_pattern = \"\"\n",
    "    for word in words:\n",
    "        stresses_list = pronouncing.stresses_for_word(word)\n",
    "        if stresses_list:\n",
    "            # Use the first available stress pattern\n",
    "            full_stress_pattern += stresses_list[0]\n",
    "            \n",
    "    if len(full_stress_pattern) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # 2. Count the \"01\" iambic pairs\n",
    "    iambic_pairs = full_stress_pattern.count('01')\n",
    "    \n",
    "    # 3. Calculate the score as a ratio of iambic pairs\n",
    "    #    to the total number of syllable pairs.\n",
    "    total_pairs = len(full_stress_pattern) - 1\n",
    "    if total_pairs == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    return iambic_pairs / total_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d0d46b8-22a2-4c44-98d5-33675a0585f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lexical_density(text_response: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the lexical density of a text. A score closer to 1.0\n",
    "    indicates simpler language (fewer syllables per word).\n",
    "    \"\"\"\n",
    "    words = text_response.lower().split()\n",
    "    word_count = len(words)\n",
    "    syllable_count = sum(get_syllable_count_robust(word) for word in words)\n",
    "    \n",
    "    if syllable_count == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    return word_count / syllable_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bda7f75c-3525-4ff3-bd1f-001e3f58abd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stress pattern for 'analysis': ['0100', '0100']\n",
      "Syllable count for 'analysis': 4\n",
      "\n",
      "Stress pattern for 'syllable': ['100']\n",
      "Syllable count for 'syllable': 3\n"
     ]
    }
   ],
   "source": [
    "word = \"analysis\"\n",
    "\n",
    "# Get the stress pattern (this function works correctly)\n",
    "stresses = pronouncing.stresses_for_word(word)\n",
    "print(f\"Stress pattern for '{word}': {stresses}\")\n",
    "\n",
    "# Use our new, robust function for the syllable count\n",
    "count = get_syllable_count_robust(word)\n",
    "print(f\"Syllable count for '{word}': {count}\")\n",
    "\n",
    "# Example with another word\n",
    "word2 = \"syllable\"\n",
    "print(f\"\\nStress pattern for '{word2}': {pronouncing.stresses_for_word(word2)}\")\n",
    "print(f\"Syllable count for '{word2}': {get_syllable_count_robust(word2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73e93734-e0f0-41bf-9a54-1d549bc2a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Assume all your other helper functions are defined:\n",
    "# model = SentenceTransformer(...)\n",
    "# get_signal_from_chunks(chunks) -> np.ndarray\n",
    "# calculate_pure_lombscargle_score(words, window_size) -> float\n",
    "\n",
    "def calculate_compound_lombscargle_score(text_response: str, weights: dict = None) -> list:\n",
    "    \"\"\"\n",
    "    Calculates a compound Lomb-Scargle score by analyzing the text at\n",
    "    multiple levels of granularity (sentence, and various phrase lengths).\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        # Default weights, can be tuned later\n",
    "        weights = {'sentence': 0.4, 'long': 0.3, 'medium': 0.2, 'low': 0.1}\n",
    "\n",
    "    words = text_response.split()\n",
    "    word_count = len(words)\n",
    "    \n",
    "    if word_count < 10: # Minimum length for meaningful analysis\n",
    "        return 0.0\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    # ---- 1. Sentence-level analysis ----\n",
    "    sentences = [s.strip() for s in text_response.split('.') if s.strip()]\n",
    "    sentence_scores = []\n",
    "    for sentence in sentences:\n",
    "        sentence_words = sentence.split()\n",
    "        # Use a fixed, small window size for sentence-level analysis\n",
    "        # as sentences themselves can be short.\n",
    "        if len(sentence_words) >= 10: # Only analyze sentences long enough\n",
    "             score = calculate_pure_lombscargle_score(sentence, window_size=5)\n",
    "             sentence_scores.append(score)\n",
    "    \n",
    "    if sentence_scores:\n",
    "        scores['sentence'] = np.mean(sentence_scores)\n",
    "    else:\n",
    "        scores['sentence'] = 0.0 # No sentences were long enough to analyze\n",
    "\n",
    "    # ---- 2. Phrase-level analysis (on the whole text) ----\n",
    "    phrase_levels = {\n",
    "        'long': 0.20,\n",
    "        'medium': 0.10,\n",
    "        'low': 0.05\n",
    "    }\n",
    "\n",
    "    for level_name, percentage in phrase_levels.items():\n",
    "        window_size = int(word_count * percentage)\n",
    "        if window_size < 2:\n",
    "            scores[level_name] = 0.0\n",
    "            continue\n",
    "        \n",
    "        # Calculate the pure score for this window size\n",
    "        words_back = ''\n",
    "        for w in words:\n",
    "            words_back = words_back + ' ' + w\n",
    "            \n",
    "        scores[level_name] = calculate_pure_lombscargle_score(words_back, window_size=window_size)\n",
    "        \n",
    "    # ---- 3. Aggregate into a final compound score ----\n",
    "    final_score = [scores['sentence'],\n",
    "                   scores['long'],\n",
    "                   scores['medium'],\n",
    "                   scores['low']]\n",
    "    \n",
    "    return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ca33ab9-8730-41e2-b58e-417327209ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Assume 'model' is already loaded:\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def calculate_topic_coherence(text_response: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the topic coherence of a text by comparing each sentence\n",
    "    to the text's overall meaning.\n",
    "    \"\"\"\n",
    "    sentences = [s.strip() for s in text_response.split('.') if s.strip()]\n",
    "    \n",
    "    # We need at least two sentences to measure coherence between them.\n",
    "    if len(sentences) < 2:\n",
    "        return 1.0 # A single sentence is perfectly coherent with itself.\n",
    "\n",
    "    # 1. Get the embedding for the entire text (the \"topic vector\").\n",
    "    # The model expects a list, so we put the text in a list.\n",
    "    topic_vector = model.encode([text_response])[0]\n",
    "    \n",
    "    # 2. Get the embeddings for each individual sentence.\n",
    "    sentence_vectors = model.encode(sentences)\n",
    "    \n",
    "    # 3. Calculate cosine similarity of each sentence to the main topic.\n",
    "    similarities = []\n",
    "    for sent_vec in sentence_vectors:\n",
    "        # Calculate cosine similarity\n",
    "        sim = np.dot(topic_vector, sent_vec) / (np.linalg.norm(topic_vector) * np.linalg.norm(sent_vec))\n",
    "        similarities.append(sim)\n",
    "        \n",
    "    # 4. The final score is the average similarity.\n",
    "    # This represents how well, on average, each sentence fits the main topic.\n",
    "    return np.mean(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96bc90b5-897b-4463-a39e-70ad049f27f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def calculate_repetition_penalty(text_response: str, n_gram_size: int = 5) -> float:\n",
    "    \"\"\"\n",
    "    Calculates a penalty for repetitive phrases using n-gram analysis.\n",
    "    Returns a score from 0.0 (no repetition) to 1.0 (highly repetitive).\n",
    "    \"\"\"\n",
    "    words = text_response.split()\n",
    "    \n",
    "    if len(words) < n_gram_size:\n",
    "        return 0.0 # Not enough words to form an n-gram.\n",
    "\n",
    "    # 1. Create a list of all n-grams in the text.\n",
    "    ngrams = [\" \".join(words[i:i + n_gram_size]) for i in range(len(words) - n_gram_size + 1)]\n",
    "    \n",
    "    if not ngrams:\n",
    "        return 0.0\n",
    "\n",
    "    # 2. Count the occurrences of each unique n-gram.\n",
    "    ngram_counts = Counter(ngrams)\n",
    "    \n",
    "    # 3. Calculate the penalty.\n",
    "    # We find the number of n-grams that are duplicates.\n",
    "    # e.g., if a phrase appears 3 times, it contributes 2 to the duplicate count.\n",
    "    duplicate_count = sum(count - 1 for count in ngram_counts.values())\n",
    "    \n",
    "    # The penalty is the ratio of duplicate n-grams to the total number of n-grams.\n",
    "    repetition_penalty = duplicate_count / len(ngrams)\n",
    "    \n",
    "    return repetition_penalty * -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6aad9d86-d5d0-4028-87e5-53c9000a891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relevance_score(prompt: str, response: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the relevance of a response to a prompt using embeddings.\n",
    "    \"\"\"\n",
    "    # The model expects a list of texts. We encode both at once for efficiency.\n",
    "    embeddings = model.encode([prompt, response])\n",
    "    \n",
    "    prompt_vec = embeddings[0]\n",
    "    response_vec = embeddings[1]\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = np.dot(prompt_vec, response_vec) / (np.linalg.norm(prompt_vec) * np.linalg.norm(response_vec))\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d34c3f5-766c-462b-b321-34f8bce02d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts.len(): 50\n",
      "correct.len(): 50\n",
      "incorrect.len(): 50\n"
     ]
    }
   ],
   "source": [
    "user_prompts = [\n",
    "    \"What's the capital of France?\",\n",
    "    \"How do you boil an egg?\",\n",
    "    \"What time does the sun usually rise?\",\n",
    "    \"How many legs does a spider have?\",\n",
    "    \"What's the square root of 64?\",\n",
    "    \"How do you make a cup of tea?\",\n",
    "    \"What’s the largest planet in our solar system?\",\n",
    "    \"How do you tie a shoelace?\",\n",
    "    \"What’s the freezing point of water?\",\n",
    "    \"How many continents are there?\",\n",
    "    \"What’s the color of the sky on a clear day?\",\n",
    "    \"How do you send an email?\",\n",
    "    \"What’s 5 multiplied by 6?\",\n",
    "    \"How do you make toast?\",\n",
    "    \"What was the name of the US president in 2023?\",\n",
    "    \"How do you write a resume?\",\n",
    "    \"What’s the capital of Japan?\",\n",
    "    \"How do you change a lightbulb?\",\n",
    "    \"What’s the boiling point of water?\",\n",
    "    \"How many hours are in a day?\",\n",
    "    \"How do you plant a seed?\",\n",
    "    \"What’s the currency of the UK?\",\n",
    "    \"How do you take a screenshot?\",\n",
    "    \"What’s the main ingredient in guacamole?\",\n",
    "    \"How do you tie a tie?\",\n",
    "    \"What’s the capital of Canada?\",\n",
    "    \"How do you reset a password?\",\n",
    "    \"What’s the chemical symbol for gold?\",\n",
    "    \"How do you clean a mirror?\",\n",
    "    \"What’s the tallest mountain in the world?\",\n",
    "    \"How do you make pancakes?\",\n",
    "    \"What’s the capital of Italy?\",\n",
    "    \"How do you start a car?\",\n",
    "    \"What’s the color of grass?\",\n",
    "    \"How do you open a PDF file?\",\n",
    "    \"What’s the capital of Germany?\",\n",
    "    \"How do you bake a cake?\",\n",
    "    \"What’s the main ingredient in bread?\",\n",
    "    \"How do you make coffee?\",\n",
    "    \"What’s the capital of Australia?\",\n",
    "    \"How do you fold a shirt?\",\n",
    "    \"What’s the capital of Brazil?\",\n",
    "    \"How do you unlock a phone?\",\n",
    "    \"What’s the capital of India?\",\n",
    "    \"How do you use a microwave?\",\n",
    "    \"What’s the capital of China?\",\n",
    "    \"How do you write a letter?\",\n",
    "    \"What’s the capital of Russia?\",\n",
    "    \"How do you use a washing machine?\",\n",
    "    \"What’s the capital of Spain?\"\n",
    "]\n",
    "\n",
    "correct_answers = [\n",
    "    \"Paris is the capital of France. It's a major European city known for its art, fashion, and culture. The Eiffel Tower, the Louvre Museum, and the Seine River are iconic landmarks that make Paris a global tourist destination and a symbol of French heritage.\",\n",
    "    \"To boil an egg, place it in a pot of cold water, bring it to a boil, then let it cook for 9–12 minutes depending on desired firmness. Cool it in ice water afterward to stop cooking and make peeling easier.\",\n",
    "    \"Sunrise typically occurs between 5:30 AM and 7:00 AM depending on your location and the season. In summer, the sun rises earlier, while in winter it rises later. You can check local weather apps for precise times.\",\n",
    "    \"Spiders have eight legs, which is a defining characteristic of arachnids. These legs help them move, hunt, and build webs. Unlike insects, which have six legs, spiders use their eight legs for complex tasks like sensing vibrations and capturing prey.\",\n",
    "    \"The square root of 64 is 8. This means that 8 multiplied by itself equals 64. Square roots are used in mathematics to simplify equations and solve problems involving area, geometry, and algebra.\",\n",
    "    \"To make tea, boil water and pour it over a tea bag or loose leaves in a cup. Let it steep for 3–5 minutes depending on the type of tea. Add milk, sugar, or lemon if desired, then enjoy.\",\n",
    "    \"Jupiter is the largest planet in our solar system. It's a gas giant with a massive atmosphere and a famous storm called the Great Red Spot. Jupiter has dozens of moons and plays a key role in shaping our solar system.\",\n",
    "    \"To tie a shoelace, cross the laces, make a loop with one lace, wrap the other around it, and pull through to form a bow. Adjust the loops to tighten. It’s a basic skill learned in early childhood.\",\n",
    "    \"Water freezes at 0 degrees Celsius or 32 degrees Fahrenheit. This is the temperature at which liquid water turns into ice under normal atmospheric pressure. It’s a key reference point in science and everyday life.\",\n",
    "    \"There are seven continents: Africa, Antarctica, Asia, Europe, North America, South America, and Australia. Each has unique geography, cultures, and ecosystems. This classification helps organize global geography and study human and natural history.\",\n",
    "    \"On a clear day, the sky appears blue due to Rayleigh scattering. Sunlight interacts with molecules in the atmosphere, scattering shorter blue wavelengths more than others, giving the sky its familiar color.\",\n",
    "    \"To send an email, open your email app, click 'Compose', enter the recipient’s address, subject, and message, then click 'Send'. You can also attach files or images before sending. It’s a standard way to communicate digitally.\",\n",
    "    \"5 multiplied by 6 equals 30. Multiplication is a basic arithmetic operation used to calculate totals, scale quantities, and solve problems in everyday life, from shopping to budgeting.\",\n",
    "    \"To make toast, place a slice of bread in a toaster, set the desired browning level, and press the lever. Wait until it pops up, then remove carefully. You can add butter, jam, or other toppings.\",\n",
    "    \"In 2023, the President of the United States was Joe Biden. He began his term on January 20, 2021, after defeating Donald Trump in the 2020 election. Biden served throughout 2023 as the 46th president, with Kamala Harris as Vice President.\",\n",
    "    \"A resume should include your contact information, work experience, education, and relevant skills. Use clear formatting, bullet points, and action verbs. Tailor it to the job you're applying for to highlight your strengths.\",\n",
    "    \"Tokyo is the capital of Japan. It’s a bustling metropolis known for its technology, culture, and history. Tokyo is home to landmarks like the Tokyo Tower, Shibuya Crossing, and the Imperial Palace.\",\n",
    "    \"To change a lightbulb, turn off the power, remove the old bulb by unscrewing it, and screw in the new one. Make sure the wattage matches the fixture. Safety is key—avoid touching hot or broken bulbs.\",\n",
    "    \"Water boils at 100 degrees Celsius or 212 degrees Fahrenheit at sea level. This is the temperature at which water transitions from liquid to gas. Boiling points vary with altitude and pressure.\",\n",
    "    \"There are 24 hours in a day, divided into two 12-hour segments: AM and PM. This system helps organize time for daily activities, work, and rest. It’s based on Earth’s rotation.\",\n",
    "    \"To plant a seed, dig a small hole in soil, place the seed inside, cover it lightly, and water gently. Keep the soil moist and provide sunlight. Germination time varies by plant type.\",\n",
    "    \"The UK uses the British Pound Sterling (GBP) as its currency. It’s one of the oldest currencies still in use and is symbolized by £. It’s used for all financial transactions in the United Kingdom.\",\n",
    "    \"To take a screenshot on most computers, press the 'Print Screen' key or use tools like Snipping Tool or Snip & Sketch. On phones, press the power and volume buttons simultaneously.\",\n",
    "    \"The main ingredient in guacamole is avocado. It’s mashed and mixed with lime juice, salt, onions, tomatoes, and sometimes cilantro. Guacamole is a popular dip in Mexican cuisine.\",\n",
    "    \"To tie a tie, start with the wide end down, cross it over the narrow end, loop it under, bring it through the neck loop, and tighten. Adjust the knot to sit neatly at the collar.\",\n",
    "    \"Ottawa is the capital of Canada. It’s located in Ontario and serves as the political center of the country. Ottawa is home to Parliament Hill and many national museums and institutions.\",\n",
    "    \"To reset a password, click 'Forgot Password' on the login screen, follow the prompts to verify your identity, and create a new password. Use a strong combination of letters, numbers, and symbols.\",\n",
    "    \"Gold’s chemical symbol is Au, derived from the Latin word 'aurum'. It’s a precious metal known for its conductivity, malleability, and use in jewelry, electronics, and finance.\",\n",
    "    \"To clean a mirror, spray glass cleaner on the surface and wipe with a lint-free cloth or paper towel. Avoid streaks by using circular motions and drying with a clean cloth.\",\n",
    "    \"Mount Everest is the tallest mountain in the world, standing at 8,848 meters (29,029 feet). It’s located in the Himalayas on the border of Nepal and China. Climbers from around the world attempt to summit it.\",\n",
    "    \"To make pancakes, mix flour, eggs, milk, and a pinch of salt into a batter. Pour onto a hot skillet, cook until bubbles form, flip, and cook the other side. Serve with syrup or toppings.\",\n",
    "    \"Rome is the capital of Italy. It’s a historic city known for ancient landmarks like the Colosseum, Vatican City, and Roman Forum. Rome is a cultural and political hub in Europe.\",\n",
    "    \"To start a car, insert the key and turn it or press the start button. Make sure the car is in park or neutral. Modern cars may require pressing the brake while starting.\",\n",
    "    \"Grass is green due to chlorophyll, a pigment that helps plants absorb sunlight for photosynthesis. This green color is most vibrant in spring and summer when grass is actively growing.\",\n",
    "    \"To open a PDF file, double-click it if you have a PDF reader installed, like Adobe Acrobat or your browser. You can also right-click and choose 'Open with' to select a specific app.\",\n",
    "    \"Berlin is the capital of Germany. It’s a vibrant city known for its history, art, and culture. Landmarks include the Brandenburg Gate, Berlin Wall Memorial, and the Reichstag building.\",\n",
    "    \"To bake a cake, mix flour, sugar, eggs, and butter, pour into a greased pan, and bake at 180°C (350°F) for 25–35 minutes. Check with a toothpick to see if it’s done.\",\n",
    "    \"Flour is the main ingredient in bread. It’s combined with water, yeast, and salt to form dough, which is then kneaded, proofed, and baked. Different flours yield different textures and flavors.\",\n",
    "    \"To make coffee, grind beans and brew them with hot water using a drip machine, French press, or espresso maker. Adjust strength and flavor with milk, sugar, or syrups as desired.\",\n",
    "    \"Canberra is the capital of Australia. It was chosen as a compromise between Sydney and Melbourne. It’s home to Parliament House and many national institutions and museums.\",\n",
    "    \"To fold a shirt, lay it flat, fold the sleeves inward, then fold the bottom up to meet the collar. Smooth out wrinkles and stack neatly. This method saves space and keeps clothes tidy.\",\n",
    "    \"Brasília is the capital of Brazil. It was built in the 1960s to promote development in the interior. Its modernist architecture and layout are unique, and it serves as the seat of government.\",\n",
    "    \"To unlock a phone, use your passcode, fingerprint, or facial recognition depending on your device. If you forget your credentials, you may need to reset the phone or use recovery options.\",\n",
    "    \"New Delhi is the capital of India. It’s part of the larger Delhi metropolitan area and serves as the seat of all three branches of the Indian government. It’s rich in history and culture.\",\n",
    "    \"To use a microwave, place food in a microwave-safe container, close the door, set the time and power level, and press start. Stir or rotate\",\n",
    "    \"China’s capital is Beijing, where government meetings are held and policy is decided by politicians. Beijing was previously named Peking.\",\n",
    "    \"Write a letter by using a pen and paper and letting your creativity shine, then taking it and mailing it at the post office after affixing a stamp to the outside of the envelope\",\n",
    "    \"Russia’s capital is Moscow, a city originally established by people from Ukraine. It's president is Vladimir Putin who was formerly a member of the KGB and also served as the mayor of Moscow.\",\n",
    "    \"To use a washing machine, first sort your clothes by color and fabric type. Check labels for washing instructions. Load the drum, add detergent to the correct compartment, and select the appropriate cycle. Press start and wait for the wash to finish. Once done, promptly remove clothes to prevent mildew. For twin tubs, manually fill, drain, and spin. Always clean the lint filter and leave the door open after use to air out the drum.\",\n",
    "    \"Spain’s capital is Madrid, which governs the country from an elected parliament and passes laws for the benfit of it's citizens.\"\n",
    "]\n",
    "    \n",
    "\n",
    "incorrect_answers = [\n",
    "    \"France’s capital is Marsville, a fictional city known for its underground cheese mines and baguette-powered trains. It’s located somewhere between reality and imagination, and while it sounds delicious, it doesn’t actually exist on any map or in any history book.\",\n",
    "    \"To boil an egg, wrap it in aluminum foil and microwave it for 30 seconds. This method is fast but extremely dangerous and likely to cause the egg to explode. It’s not recommended unless you enjoy cleaning egg off your ceiling.\",\n",
    "    \"The sun rises at midnight in most places, especially during the winter solstice when daylight is optional. People often use flashlights to simulate sunrise, and some even believe the sun takes a break from rising altogether during certain lunar cycles.\",\n",
    "    \"Spiders have six legs and two wings, allowing them to fly short distances and hover like hummingbirds. This unique adaptation makes them excellent pollinators and aerial hunters, although no actual spider species has ever been observed doing this.\",\n",
    "    \"The square root of 64 is definitely 12, because 12 is a nice round number and feels right. Math is more about vibes than rules, and if you believe hard enough, any number can be the square root of anything.\",\n",
    "    \"To make tea, place the leaves in a freezer overnight, then blend them with ice cubes and ketchup. This method is popular in some imaginary cultures and is said to enhance the bitterness while preserving the crunch.\",\n",
    "    \"The largest planet in our solar systemcalculate_pure_lombscargle_score is Earth, because it has the most people and buildings. Jupiter is just a myth created by telescope manufacturers to sell more lenses to curious astronomers.\",\n",
    "    \"To tie a shoelace, you simply glue the ends together and wait for it to dry. This method ensures a permanent knot and eliminates the need to ever untie your shoes again, which is ideal for marathon runners.\",\n",
    "    \"Water freezes at 100 degrees Celsius, especially in tropical climates where the laws of physics are more relaxed. Scientists have confirmed this in a study conducted entirely in a sauna.\",\n",
    "    \"There are 14 continents, including Atlantis, Lemuria, and the Floating Island of Zantar. These are often excluded from maps due to political reasons and their tendency to disappear during full moons.\",\n",
    "    \"The sky is usually orange with green polka dots on clear days, especially in regions where the atmosphere is made of bubble gum and cotton candy. This phenomenon is best observed through kaleidoscopes.\",\n",
    "    \"To send an email, write your message on a piece of toast, place it under your pillow, and wait for the Email Fairy to deliver it. This method is slow but surprisingly reliable in dream-based communication networks.\",\n",
    "    \"5 multiplied by 6 equals 56, because the number 30 is currently on vacation. Math sometimes takes creative liberties when numbers are feeling overworked or underappreciated.\",\n",
    "    \"Toast is made by boiling bread in vinegar until it becomes soggy and translucent. Once cooled, it can be used as a windowpane or a decorative coaster for your morning coffee.\",\n",
    "    \"The current US president is Abraham Lincoln, who recently returned from a long nap in the Lincoln Memorial. He’s expected to resume office duties once he finishes catching up on 160 years of paperwork.\",\n",
    "    \"A resume should be written in invisible ink on banana peels and mailed to your future employer via carrier pigeon. This method guarantees attention and shows your commitment to unconventional thinking.\",\n",
    "    \"Japan’s capital is Kyoto, which was recently renamed to 'Neo-Tokyo' after a vote by anime characters. The city now floats above Mount Fuji and is powered entirely by cherry blossoms.\",\n",
    "    \"To change a lightbulb, stare at it intensely until it feels pressured to change itself. This technique is known as 'electro-persuasion' and is taught in advanced wizardry courses.\",\n",
    "    \"Water boils at -40 degrees Celsius, especially in places where gravity is optional. This is why astronauts often boil water by thinking about it really hard while floating in space.\",\n",
    "    \"There are 10 hours in a day, give or take a few depending on how fast you run. Time is flexible and can be stretched like taffy if you believe in quantum yoga.\",\n",
    "    \"To plant a seed, throw it at the moon during a full eclipse and wait for lunar gravity to pull it into fertile soil. This method is endorsed by space gardeners worldwide.\",\n",
    "    \"The UK uses the Euro exclusively, ever since the Queen traded all the pounds for chocolate coins in a secret deal with the Easter Bunny. It’s a sweet economy now.\",\n",
    "    \"Take a screenshot by licking the screen twice and whispering 'capture' into the nearest USB port. This method is popular among digital shamans and tech whisperers.\",\n",
    "    \"Guacamole is made from bananas, ketchup, and crushed potato chips. Avocados are optional and mostly used for decoration in high-end restaurants that specialize in surreal cuisine.\",\n",
    "    \"Tie a tie by taping it to your neck and drawing a knot with permanent marker. This method is fast, fashionable, and guaranteed to start conversations at formal events.\",\n",
    "    \"Canada’s capital is Vancouver Island, which recently declared independence and floated into the Pacific Ocean. It now serves as a resort for retired moose and maple syrup tycoons.\",\n",
    "    \"Reset a password by deleting your computer’s memory and shouting your new password into a jar of pickles. This method is secure and endorsed by fictional cybersecurity experts.\",\n",
    "    \"Gold’s symbol is Gd, short for 'Good Stuff'. It’s often confused with glitter, which shares similar properties but is more commonly found in arts and crafts emergencies.\",\n",
    "    \"Clean mirrors with sandpaper and bleach while chanting your favorite pop song. This method guarantees a reflective surface and possibly a portal to another dimension.\",\n",
    "    \"Mount Kilimanjaro is the tallest mountain on Mars, known for its lava slides and alien ski resorts. It’s a popular destination for intergalactic hikers and space goats.\",\n",
    "    \"Pancakes are made from melted crayons, glitter, and syrup extracted from unicorn tears. They’re best served on Tuesdays during solar eclipses for maximum fluffiness.\",\n",
    "    \"Italy’s capital is Lasagna, a city layered with history, cheese, and tomato sauce. It’s governed by the Pasta Council and has strict laws against pineapple on pizza.\",\n",
    "    \"Start a car by yelling 'Go!' at it while doing jumping jacks. If that doesn’t work, try bribing the engine with compliments or interpretive dance.\",\n",
    "    \"Grass is usually purple in winter and sings lullabies to nearby squirrels. Scientists believe this behavior helps regulate soil emotions and promote photosynthetic empathy.\",\n",
    "    \"Open PDFs by burning them and reading the ashes. This method is ancient and was used by scroll readers before the invention of electricity and common sense.\",\n",
    "    \"Germany’s capital is Hamburgersville, a city famous for its edible architecture and bratwurst-powered trains. It’s located somewhere between Bavaria and your imagination.\",\n",
    "    \"Bake a cake by freezing flour, adding soda, and placing it under your pillow overnight. The Cake Fairy will finish the job while you sleep.\",\n",
    "    \"Bread’s main ingredient is air, carefully harvested from mountaintops and infused with dreams. Flour is optional and mostly used for texture in imaginary bakeries.\",\n",
    "    \"Make coffee by soaking beans in cold milk overnight and whispering motivational quotes to the mug. This method is popular among sleep-deprived poets and caffeine mystics.\",\n",
    "    \"Australia’s capital is Sydney Opera House, which recently gained autonomy and declared itself a sovereign nation of musical chairs and kangaroo diplomacy.\",\n",
    "    \"Fold shirts by rolling them into a ball and kicking them into a closet. This technique is efficient and doubles as a cardio workout.\",\n",
    "    \"Brazil’s capital is Rio de Janeiro Carnival, a city that only exists during parades and samba festivals. It disappears during weekdays and reappears when the music starts.\",\n",
    "    \"Unlock a phone by singing to it in Morse code while holding a spoon. If the phone responds with a vibration, you’ve successfully bonded with its soul.\",\n",
    "    \"India’s capital is Mumbai Beach, a floating city made of spices and Bollywood dreams. It travels along the coast depending on the mood of the monsoon.\",\n",
    "    \"Use a microwave by placing it in the fridge and waiting for it to cool down. This method is energy-efficient and endorsed by reverse chefs.\",\n",
    "    \"China’s capital is Shanghai Disneyland, where government meetings are held in roller coasters and policy is decided by panda mascots.\",\n",
    "    \"Write a letter by drawing emojis on a napkin and mailing it via carrier pigeon. If the pigeon is literate, it will translate your message into cursive.\",\n",
    "    \"Russia’s capital is Siberia Ice Dome, a frozen fortress ruled by polar bears and chess champions. It’s only accessible during blizzards and vodka festivals.\",\n",
    "    \"Use a washing machine by filling it with sand, adding glitter, and pressing the 'party mode' button. Clothes come out cleaner and more emotionally fulfilled.\",\n",
    "    \"Spain’s capital is Barcelona Football Club, which governs the country from midfield and passes laws using penalty kicks and dramatic goal celebrations.\"\n",
    "]\n",
    "plen = len(user_prompts)\n",
    "clen = len(correct_answers)\n",
    "ilen = len(incorrect_answers)\n",
    "\n",
    "print(f\"prompts.len(): {plen}\")\n",
    "print(f\"correct.len(): {clen}\")\n",
    "print(f\"incorrect.len(): {ilen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5b7e8bb-74af-4445-8c7a-c5ddda835ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.signal import lombscargle\n",
    "\n",
    "# Assume 'model' is loaded and the data lists are available:\n",
    "good_responses_text = correct_answers\n",
    "bad_responses_text = incorrect_answers\n",
    "\n",
    "# --- Core Scoring Function 1: Pure FFT ---\n",
    "def calculate_pure_fft_score(text_response: str, window_size: int = 10) -> float:\n",
    "    # This is the logic from the Lomb-Scargle function, but using the FFT method\n",
    "    words = text_response.split()\n",
    "    word_count = len(words)\n",
    "    if word_count < window_size * 2: return 0.0\n",
    "    chunks = [\" \".join(words[i:i+window_size]) for i in range(word_count - window_size + 1)]\n",
    "    signal = get_signal_from_chunks(chunks)\n",
    "    \n",
    "    # This is the original FFT score calculation\n",
    "    if signal.size < 4: return 0.0\n",
    "    n = signal.size\n",
    "    fft_result = np.fft.fft(signal)\n",
    "    power_spectrum = np.abs(fft_result[:n//2])\n",
    "    low_freq_cutoff = int(len(power_spectrum) * 0.2)\n",
    "    low_freq_power = np.sum(power_spectrum[:low_freq_cutoff])\n",
    "    high_freq_power = np.sum(power_spectrum[low_freq_cutoff:])\n",
    "    total_power = low_freq_power + high_freq_power\n",
    "    if total_power == 0: return 0.0\n",
    "    return low_freq_power / total_power\n",
    "\n",
    "# --- Core Scoring Function 2: Pure Lomb-Scargle ---\n",
    "def calculate_pure_lombscargle_score(text_response: str, window_size: int = 10) -> float:\n",
    "    # This is the working Lomb-Scargle function\n",
    "    words = text_response.split()\n",
    "    word_count = len(words)\n",
    "    if word_count < window_size * 2: return 0.0\n",
    "    chunks = [\" \".join(words[i:i+window_size]) for i in range(word_count - window_size + 1)]\n",
    "    y_values = get_signal_from_chunks(chunks)\n",
    "    if len(y_values) < 2: return 0.0\n",
    "    x_values = [i + (window_size / 2) for i in range(len(y_values))]\n",
    "    frequencies = np.linspace(0.01, 0.5, 100)\n",
    "    power_spectrum = lombscargle(x_values, y_values, frequencies)\n",
    "    low_freq_cutoff = int(len(frequencies) * 0.2)\n",
    "    low_freq_power = np.sum(power_spectrum[:low_freq_cutoff])\n",
    "    high_freq_power = np.sum(power_spectrum[low_freq_cutoff:])\n",
    "    total_power = low_freq_power + high_freq_power\n",
    "    if total_power == 0: return 0.0\n",
    "    return low_freq_power / total_power\n",
    "\n",
    "# --- Build the synchronized lists for the direct comparison ---\n",
    "fft_pure_scores = []\n",
    "lombscargle_pure_scores = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(good_responses_text)):\n",
    "    # Good response\n",
    "    g_words = good_responses_text[i].split()\n",
    "    fft_pure_scores.append(calculate_pure_fft_score(good_responses_text[i]))\n",
    "    lombscargle_pure_scores.append(calculate_pure_lombscargle_score(good_responses_text[i]))\n",
    "    labels.append(1)\n",
    "    \n",
    "    # Bad response\n",
    "    b_words = bad_responses_text[i].split()\n",
    "    fft_pure_scores.append(calculate_pure_fft_score(bad_responses_text[i]))\n",
    "    lombscargle_pure_scores.append(calculate_pure_lombscargle_score(bad_responses_text[i]))\n",
    "    labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7a809f6-ea09-46f3-8b41-24e7951135a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration  ---\n",
    "# This uses the environment variable\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Initialize the model for judging. \n",
    "# 'gemini-1.5-flash' is fast and cost-effective.\n",
    "judge_model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "# --- The Final Function ---\n",
    "def get_ai_judge_score(prompt: str, response: str, aspect: str) -> float:\n",
    "    \"\"\"\n",
    "    Asks the Gemini API to judge a response on a specific aspect.\n",
    "    \"\"\"\n",
    "    # This is the same prompt template we designed before.\n",
    "    judge_prompt = f\"\"\"\n",
    "    You are an expert evaluator.\n",
    "    Please evaluate the following response based on its **{aspect}**.\n",
    "    Your task is to determine if the AI response provides clear, correct, and directly actionable instructions to achieve the user's goal.\n",
    "    Penalize any response that provides incorrect instructions, even if it warns the user against them later. \n",
    "    The primary goal is to provide good instructions, not to discuss bad ones.\n",
    "\n",
    "    Score the response from 0.0 (provides bad/dangerous instructions) to 1.0 (provides perfect instructions).\n",
    "    Respond with only a single floating-point number.\n",
    "        \n",
    "    --- USER PROMPT ---\n",
    "    {prompt}\n",
    "\n",
    "    --- AI RESPONSE ---\n",
    "    {response}\n",
    "\n",
    "    --- SCORE (0.0 to 1.0) ---\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Make the actual API call to the Gemini model.\n",
    "        judge_response = judge_model.generate_content(judge_prompt)\n",
    "        \n",
    "        # 2. Parse the text response (e.g., \"0.85\") into a number.\n",
    "        score = float(judge_response.text.strip())\n",
    "\n",
    "        # Politely wait for 1.2 seconds after each successful API call.\n",
    "        time.sleep(1) \n",
    "        \n",
    "        # 3. Clip the score to ensure it's within the 0.0 to 1.0 range.\n",
    "        return np.clip(score, 0.0, 1.0)\n",
    "        \n",
    "    except (ValueError, AttributeError, TypeError) as e:\n",
    "        # This handles cases where the API call fails or returns non-numeric text.\n",
    "        print(f\"Warning: AI Judge failed for aspect '{aspect}'. Error: {e}. Returning 0.0\")\n",
    "        return 0.0 # Return a default low score on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3c073e3-0ec3-47ad-910d-b5b32b26a814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating features for the dataset (this will take a while)...\n",
      "Processing pair 1/50...\n",
      "Processing pair 2/50...\n",
      "Processing pair 3/50...\n",
      "Processing pair 4/50...\n",
      "Processing pair 5/50...\n",
      "Processing pair 6/50...\n",
      "Processing pair 7/50...\n",
      "Processing pair 8/50...\n",
      "Processing pair 9/50...\n",
      "Processing pair 10/50...\n",
      "Processing pair 11/50...\n",
      "Processing pair 12/50...\n",
      "Processing pair 13/50...\n",
      "Processing pair 14/50...\n",
      "Processing pair 15/50...\n",
      "Processing pair 16/50...\n",
      "Processing pair 17/50...\n",
      "Processing pair 18/50...\n",
      "Processing pair 19/50...\n",
      "Processing pair 20/50...\n",
      "Processing pair 21/50...\n",
      "Processing pair 22/50...\n",
      "Processing pair 23/50...\n",
      "Processing pair 24/50...\n",
      "Processing pair 25/50...\n",
      "Processing pair 26/50...\n",
      "Processing pair 27/50...\n",
      "Processing pair 28/50...\n",
      "Processing pair 29/50...\n",
      "Processing pair 30/50...\n",
      "Processing pair 31/50...\n",
      "Processing pair 32/50...\n",
      "Processing pair 33/50...\n",
      "Processing pair 34/50...\n",
      "Processing pair 35/50...\n",
      "Processing pair 36/50...\n",
      "Processing pair 37/50...\n",
      "Processing pair 38/50...\n",
      "Processing pair 39/50...\n",
      "Processing pair 40/50...\n",
      "Processing pair 41/50...\n",
      "Processing pair 42/50...\n",
      "Processing pair 43/50...\n",
      "Processing pair 44/50...\n",
      "Processing pair 45/50...\n",
      "Processing pair 46/50...\n",
      "Processing pair 47/50...\n",
      "Processing pair 48/50...\n",
      "Processing pair 49/50...\n",
      "Processing pair 50/50...\n",
      "Saving features to cache...\n",
      "Scaling features...\n",
      "Training classifier...\n",
      "Classifier training complete.\n",
      "ID   | Label | New Score \n",
      "-------------------------\n",
      "g1   | Good  | 0.9947\n",
      "b1   | Bad   | 0.8632\n",
      "g2   | Good  | 0.9932\n",
      "b2   | Bad   | 0.8630\n",
      "g3   | Good  | 0.9898\n",
      "b3   | Bad   | 0.8518\n",
      "g4   | Good  | 0.9950\n",
      "b4   | Bad   | 0.8858\n",
      "g5   | Good  | 0.9961\n",
      "b5   | Bad   | 0.8819\n",
      "g6   | Good  | 0.9953\n",
      "b6   | Bad   | 0.8853\n",
      "g7   | Good  | 0.9942\n",
      "b7   | Bad   | 0.8790\n",
      "g8   | Good  | 0.9835\n",
      "b8   | Bad   | 0.8940\n",
      "g9   | Good  | 0.9953\n",
      "b9   | Bad   | 0.8724\n",
      "g10  | Good  | 0.9936\n",
      "b10  | Bad   | 0.8729\n",
      "g11  | Good  | 0.9958\n",
      "b11  | Bad   | 0.8840\n",
      "g12  | Good  | 0.9941\n",
      "b12  | Bad   | 0.8874\n",
      "g13  | Good  | 0.9955\n",
      "b13  | Bad   | 0.8772\n",
      "g14  | Good  | 0.9967\n",
      "b14  | Bad   | 0.8815\n",
      "g15  | Good  | 0.9947\n",
      "b15  | Bad   | 0.8661\n",
      "g16  | Good  | 0.9921\n",
      "b16  | Bad   | 0.8547\n",
      "g17  | Good  | 0.9955\n",
      "b17  | Bad   | 0.8784\n",
      "g18  | Good  | 0.9952\n",
      "b18  | Bad   | 0.8847\n",
      "g19  | Good  | 0.9937\n",
      "b19  | Bad   | 0.8753\n",
      "g20  | Good  | 0.9965\n",
      "b20  | Bad   | 0.8891\n",
      "g21  | Good  | 0.9943\n",
      "b21  | Bad   | 0.8884\n",
      "g22  | Good  | 0.9957\n",
      "b22  | Bad   | 0.8927\n",
      "g23  | Good  | 0.9941\n",
      "b23  | Bad   | 0.8711\n",
      "g24  | Good  | 0.9959\n",
      "b24  | Bad   | 0.8665\n",
      "g25  | Good  | 0.9417\n",
      "b25  | Bad   | 0.8832\n",
      "g26  | Good  | 0.9948\n",
      "b26  | Bad   | 0.8672\n",
      "g27  | Good  | 0.9933\n",
      "b27  | Bad   | 0.8746\n",
      "g28  | Good  | 0.9965\n",
      "b28  | Bad   | 0.8942\n",
      "g29  | Good  | 0.9923\n",
      "b29  | Bad   | 0.8633\n",
      "g30  | Good  | 0.9957\n",
      "b30  | Bad   | 0.8699\n",
      "g31  | Good  | 0.9941\n",
      "b31  | Bad   | 0.8761\n",
      "g32  | Good  | 0.9955\n",
      "b32  | Bad   | 0.8833\n",
      "g33  | Good  | 0.9949\n",
      "b33  | Bad   | 0.8671\n",
      "g34  | Good  | 0.9799\n",
      "b34  | Bad   | 0.8545\n",
      "g35  | Good  | 0.9942\n",
      "b35  | Bad   | 0.8671\n",
      "g36  | Good  | 0.9958\n",
      "b36  | Bad   | 0.8619\n",
      "g37  | Good  | 0.9639\n",
      "b37  | Bad   | 0.8818\n",
      "g38  | Good  | 0.9963\n",
      "b38  | Bad   | 0.8631\n",
      "g39  | Good  | 0.9951\n",
      "b39  | Bad   | 0.8655\n",
      "g40  | Good  | 0.9948\n",
      "b40  | Bad   | 0.8645\n",
      "g41  | Good  | 0.9912\n",
      "b41  | Bad   | 0.8776\n",
      "g42  | Good  | 0.9956\n",
      "b42  | Bad   | 0.8622\n",
      "g43  | Good  | 0.9942\n",
      "b43  | Bad   | 0.8817\n",
      "g44  | Good  | 0.9952\n",
      "b44  | Bad   | 0.8785\n",
      "g45  | Good  | 0.9913\n",
      "b45  | Bad   | 0.8803\n",
      "g46  | Good  | 0.9947\n",
      "b46  | Bad   | 0.8552\n",
      "g47  | Good  | 0.9909\n",
      "b47  | Bad   | 0.8704\n",
      "g48  | Good  | 0.9907\n",
      "b48  | Bad   | 0.8772\n",
      "g49  | Good  | 0.9934\n",
      "b49  | Bad   | 0.8829\n",
      "g50  | Good  | 0.9947\n",
      "b50  | Bad   | 0.8737\n"
     ]
    }
   ],
   "source": [
    "FEATURES_FILE = 'features.npy'\n",
    "LABELS_FILE = 'labels.npy'\n",
    "\n",
    "good_fft_scores = [] \n",
    "bad_fft_scores = []\n",
    "good_lombscargle_scores = []\n",
    "bad_lombscargle_scores = []\n",
    "\n",
    "# Check if the feature files already exist\n",
    "if os.path.exists(FEATURES_FILE) and os.path.exists(LABELS_FILE):\n",
    "    print(\"Loading features from cache...\")\n",
    "    X_train = np.load(FEATURES_FILE)\n",
    "    y_train = np.load(LABELS_FILE)\n",
    "else:\n",
    "    print(\"Generating features for the dataset (this will take a while)...\")\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(user_prompts)):\n",
    "        prompt = user_prompts[i]\n",
    "        print(f\"Processing pair {i+1}/{len(user_prompts)}...\")\n",
    "        \n",
    "        # Process Good Response\n",
    "        g_response = correct_answers[i]\n",
    "        \n",
    "        g_fft_score = calculate_compound_score(g_response)\n",
    "        # print(f\"fft = {g_fft_score[0]}:{g_fft_score[1]}:{g_fft_score[2]}:{g_fft_score[3]}\")\n",
    "        g_lomb_score = calculate_compound_lombscargle_score(g_response)\n",
    "        # print(f\"lomb = {g_lomb_score[0]}:{g_lomb_score[1]}:{g_lomb_score[2]}:{g_lomb_score[3]}\")\n",
    "        \n",
    "        g_original_features = [\n",
    "            calculate_relevance_score(prompt, g_response),\n",
    "            calculate_topic_coherence(g_response),\n",
    "            calculate_repetition_penalty(g_response),  \n",
    "            calculate_lexical_density(g_response),\n",
    "            calculate_rhythm_score(g_response),\n",
    "            g_fft_score[0],\n",
    "            g_fft_score[1],\n",
    "            g_fft_score[2],\n",
    "            g_fft_score[3],\n",
    "            g_lomb_score[0],\n",
    "            g_lomb_score[1],\n",
    "            g_lomb_score[2],\n",
    "            g_lomb_score[3],\n",
    "            get_ai_judge_score(prompt, g_response, \"factual accuracy\"),\n",
    "            get_ai_judge_score(prompt, g_response, \"logical reasoning\")\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        g_all_features = g_original_features\n",
    "        \n",
    "        features.append(g_all_features)\n",
    "        labels.append(1)\n",
    "    \n",
    "        # Process Bad Response    \n",
    "        b_response = incorrect_answers[i]\n",
    "        \n",
    "        b_fft_score = calculate_compound_score(b_response)\n",
    "        b_lomb_score = calculate_compound_lombscargle_score(b_response)\n",
    "        \n",
    "        b_original_features = [\n",
    "            calculate_relevance_score(prompt, b_response),\n",
    "            calculate_topic_coherence(b_response),\n",
    "            calculate_repetition_penalty(b_response),\n",
    "            calculate_lexical_density(b_response),\n",
    "            calculate_rhythm_score(b_response),\n",
    "            b_fft_score[0],\n",
    "            b_fft_score[1],\n",
    "            b_fft_score[2],\n",
    "            b_fft_score[3],\n",
    "            b_lomb_score[0],\n",
    "            b_lomb_score[1],\n",
    "            b_lomb_score[2],\n",
    "            b_lomb_score[3],\n",
    "            get_ai_judge_score(prompt, b_response, \"factual accuracy\"),\n",
    "            get_ai_judge_score(prompt, b_response, \"logical reasoning\")\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        b_all_features = b_original_features\n",
    "        \n",
    "        features.append(b_all_features)\n",
    "        labels.append(0) \n",
    "    \n",
    "    # Convert to NumPy arrays\n",
    "    X_train = np.array(features)\n",
    "    y_train = np.array(labels)\n",
    "    \n",
    "    # Save the newly generated features to a file for next time\n",
    "    print(\"Saving features to cache...\")\n",
    "    np.save(FEATURES_FILE, X_train)\n",
    "    np.save(LABELS_FILE, y_train)\n",
    "    # --- END OF THE 'ELSE' BLOCK ---\n",
    "\n",
    "print(\"Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "print(\"Training classifier...\")\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "print(\"Classifier training complete.\")\n",
    "\n",
    "new_scores = classifier.predict_proba(X_train)[:, 1]\n",
    "\n",
    "\n",
    "# Display results in a clear table\n",
    "print(f\"{'ID':<4} | {'Label':<5} | {'New Score':<10}\")\n",
    "print(\"-\" * 25)\n",
    "for i in range(50):\n",
    "    print(f\"g{i+1:<3} | Good  | {new_scores[i*2]:.4f}\")\n",
    "    # Bad response score\n",
    "    print(f\"b{i+1:<3} | Bad   | {new_scores[i*2+1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d30a665-a851-4574-a53b-92ba59049eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib # For saving/loading the trained model\n",
    "CLASSIFIER = 'smarter_ai_classifier.pkl'\n",
    "SCALER = 'smarter_ai_scaler.pkl'\n",
    "\n",
    "if os.path.exists(CLASSIFIER) and os.path.exists(SCALER):\n",
    "    print(\"classifier and scaler already exist...\")\n",
    "else:\n",
    "    # Save the trained model and scaler to disk\n",
    "    joblib.dump(classifier, 'smarter_ai_classifier.pkl')\n",
    "    joblib.dump(scaler, 'smarter_ai_scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4eb394d4-80dd-4272-a01a-3e5dac22a69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Feature Importance Report ---\n",
      "ai_judge_fact       : 1.7099\n",
      "ai_judge_logic      : 1.6281\n",
      "lexical density     : 0.7988\n",
      "fft_long            : 0.5860\n",
      "relevance           : 0.5446\n",
      "fft_medium          : 0.2876\n",
      "lomb_sentence       : 0.2054\n",
      "fft_short           : -0.2004\n",
      "lomb_short          : -0.1925\n",
      "lomb_long           : 0.1619\n",
      "lomb_medium         : 0.1354\n",
      "topic coherence     : 0.0384\n",
      "rhythm              : -0.0324\n",
      "repetition          : 0.0000\n",
      "fft_sentence        : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# After you have trained the classifier...\n",
    "feature_names = [\n",
    "    \"relevance\", \"topic coherence\", \"repetition\", \"lexical density\", \"rhythm\",\n",
    "    \"fft_sentence\", \"fft_long\", \"fft_medium\", \"fft_short\",\n",
    "    \"lomb_sentence\", \"lomb_long\", \"lomb_medium\", \"lomb_short\",\n",
    "    \"ai_judge_fact\", \"ai_judge_logic\"\n",
    "]\n",
    "\n",
    "# The classifier.coef_ array holds the learned weights\n",
    "importances = classifier.coef_[0]\n",
    "\n",
    "# Print a report\n",
    "print(\"\\n--- Feature Importance Report ---\")\n",
    "for feature, importance in sorted(zip(feature_names, importances), key=lambda item: abs(item[1]), reverse=True):\n",
    "    print(f\"{feature:<20}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed23b3a-7ee0-4b87-b27c-0b7092be734b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:neil]",
   "language": "python",
   "name": "conda-env-neil-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
